{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d2d495",
   "metadata": {},
   "source": [
    "\n",
    "# AutoGluon 분류 파이프라인 (Stratified K-Fold, 셀별 실행 · 함수형태 X)\n",
    "\n",
    "**목표**: `train.csv`(타깃: `target`)로 학습 → `test.csv` 분류 결과 생성  \n",
    "**요구사항 반영**\n",
    "- Stratified K-Fold (OOF & 폴드 성능)\n",
    "- **각 기능별 셀**로 구현(함수 정의 없이 바로 실행)\n",
    "- 데이터 전처리:\n",
    "  - **G1 그룹**: `[\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]` → 표준화(평균0, 분산1) → **PCA(설명분산 0.95)** → `PC_G1_1, PC_G1_2, ...`만 남기고 G1 원본 6개 변수 **삭제**\n",
    "  - **PAIR_GROUPS(쌍 그룹)**: 각 쌍에서 대표 1개만 남김. **대표 선택 기준**: 결측률↓, mutual information(↑), ANOVA-F(↑)\n",
    "- AutoGluon으로 학습/검증/추론\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2c456a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1. Imports & Config ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "    AUTOGluon_OK = True\n",
    "except Exception as e:\n",
    "    AUTOGluon_OK = False\n",
    "    print('[WARN] AutoGluon import 실패:', e)\n",
    "\n",
    "# 경로/설정\n",
    "DATA_DIR = Path('.')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "TARGET_COL = 'target'\n",
    "ID_COL = None  # 예: 'id' (없으면 None)\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "PRESETS = 'medium_quality'\n",
    "TIME_LIMIT = 1800  # 초(폴드당)\n",
    "\n",
    "SAVE_ROOT = Path('./ag_cv_models')\n",
    "OOF_PATH  = Path('./oof_predictions.csv')\n",
    "FULL_DIR  = Path('./ag_full_model')\n",
    "SUB_PATH  = Path('./submission.csv')\n",
    "\n",
    "# 그룹 정의\n",
    "G1 = [\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]\n",
    "VAR_RATIO = 0.95  # PCA 설명분산 임계\n",
    "\n",
    "# 일단 여기선 별로 사용이 안 됨.\n",
    "# 쌍 변수 그룹 (필요시 더 추가)\n",
    "PAIR_GROUPS = [\n",
    "    [\"X_04\",\"X_39\"],\n",
    "    [\"X_06\",\"X_45\"],\n",
    "    ####################\n",
    "    [\"X_10\",\"X_17\"],\n",
    "    [\"X_07\",\"X_33\"],\n",
    "    [\"X_12\",\"X_21\"],\n",
    "    [\"X_26\",\"X_30\"],\n",
    "    [\"X_38\",\"X_47\"],\n",
    "    \n",
    "    # [\"X_05\",\"X_25\"], ...  # 예시로 더 넣을 수 있음\n",
    "]\n",
    "\n",
    "# 재현성\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('[INFO] Config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6baf9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train.shape = (21693, 54)\n",
      "[INFO] test.shape  = (15004, 53)\n",
      "[INFO] 클래스 수: 21\n",
      "[INFO] 예시: ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '3', '4', '5', '6', '7', '8']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 2. Load train/test, 타깃 점검 ===\n",
    "assert TRAIN_PATH.exists(), f'train.csv가 {TRAIN_PATH} 에 없습니다.'\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "print('[INFO] train.shape =', train.shape)\n",
    "assert TARGET_COL in train.columns, f'{TARGET_COL} 컬럼이 train에 없습니다.'\n",
    "\n",
    "if TEST_PATH.exists():\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print('[INFO] test.shape  =', test.shape)\n",
    "else:\n",
    "    test = None\n",
    "    print('[WARN] test.csv 가 없어 추론 셀은 스킵될 수 있습니다.')\n",
    "\n",
    "# 타깃 클린업(문자열 통일 + 결측 제거)\n",
    "y = train[TARGET_COL].astype(str)\n",
    "na_mask = y.isna() | (y.str.lower()=='nan') | (y=='None') | (y=='')\n",
    "if na_mask.any():\n",
    "    print(f'[WARN] 타깃 결측/유사결측 {na_mask.sum()}건 발견 → 해당 행 제거')\n",
    "    train = train.loc[~na_mask].reset_index(drop=True)\n",
    "    y = train[TARGET_COL].astype(str)\n",
    "\n",
    "ALL_CLASSES = sorted(y.unique())\n",
    "print('[INFO] 클래스 수:', len(ALL_CLASSES))\n",
    "print('[INFO] 예시:', ALL_CLASSES[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06254dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 대표 선택 결과: {('X_04', 'X_39'): 'X_04', ('X_06', 'X_45'): 'X_06', ('X_10', 'X_17'): 'X_17', ('X_07', 'X_33'): 'X_07', ('X_12', 'X_21'): 'X_12', ('X_26', 'X_30'): 'X_26', ('X_38', 'X_47'): 'X_38'}\n",
      "[INFO] 삭제 대상(대표 아닌 변수): ['X_39', 'X_45', 'X_10', 'X_33', 'X_21', 'X_30', 'X_47']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3. PAIR_GROUPS 전역 대표 변수 선택 (train 기준) ===\n",
    "# - 결측률 낮을수록 좋음\n",
    "# - mutual information 높을수록 좋음\n",
    "# - ANOVA-F 높을수록 좋음\n",
    "# 점수 = (-결측률) 정규화 + (MI) 정규화 + (F) 정규화  (각 1/3 가중)\n",
    "\n",
    "# 고상관 쌍(PAIR_GROUPS)의 각 쌍에서 대표 변수 1개만 남기기 위해, 두 변수의 “품질+유용성”을 점수로 계산해 비교하는 로직\n",
    "# -> 39, 45 삭제\n",
    "\n",
    "rep_map = {}         # {('X_04','X_39'): 'X_04', ...}\n",
    "to_drop_pairs = []   # 대표가 아닌 변수 목록\n",
    "\n",
    "# 스코어 계산을 위한 임시 데이터(결측 채움: 수치 median) 구성\n",
    "num_cols = train.drop(columns=[TARGET_COL]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "tmp = train.copy()\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    tmp[num_cols] = imputer_num.fit_transform(tmp[num_cols])\n",
    "\n",
    "# 각 쌍에 대해 점수 산출\n",
    "for pair in PAIR_GROUPS:\n",
    "    pair = [c for c in pair if c in train.columns and c != TARGET_COL]\n",
    "    if len(pair) != 2:\n",
    "        print(f'[WARN] 쌍 {pair} 중 유효 컬럼이 2개가 아닙니다. 스킵.')\n",
    "        continue\n",
    "\n",
    "    cols_ok = []\n",
    "    scores = []\n",
    "    for col in pair:\n",
    "        # 결측률 (원본 기준)\n",
    "        miss_rate = train[col].isna().mean()\n",
    "\n",
    "        # MI/F는 수치형에서만 바로 계산. 수치가 아닐 경우 임시 변환(범주→순번 인코딩)\n",
    "        s = tmp[col]\n",
    "        if not np.issubdtype(s.dtype, np.number):\n",
    "            # 간단 라벨 인코딩\n",
    "            s = s.astype('category').cat.codes\n",
    "\n",
    "        X_ = s.values.reshape(-1, 1)\n",
    "        y_ = train[TARGET_COL].astype(str).values\n",
    "\n",
    "        # MI (discrete target)\n",
    "        try:\n",
    "            mi = mutual_info_classif(X_, y_, discrete_features=True, random_state=RANDOM_STATE)[0]\n",
    "        except Exception:\n",
    "            mi = 0.0\n",
    "\n",
    "        # ANOVA-F\n",
    "        try:\n",
    "            f, _ = f_classif(X_, pd.factorize(y_)[0])\n",
    "            f = float(f[0])\n",
    "        except Exception:\n",
    "            f = 0.0\n",
    "\n",
    "        cols_ok.append(col)\n",
    "        scores.append({'col': col, 'miss': miss_rate, 'mi': mi, 'f': f})\n",
    "\n",
    "    if len(scores) != 2:\n",
    "        print(f'[WARN] {pair} 점수 계산 실패(스킵)')\n",
    "        continue\n",
    "\n",
    "    # 정규화\n",
    "    miss_vals = np.array([s['miss'] for s in scores])\n",
    "    mi_vals   = np.array([s['mi']   for s in scores])\n",
    "    f_vals    = np.array([s['f']    for s in scores])\n",
    "\n",
    "    def norm(v):\n",
    "        v = v.astype(float)\n",
    "        if np.allclose(v.max(), v.min()):\n",
    "            return np.zeros_like(v)\n",
    "        return (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "    miss_n = norm(1 - miss_vals)  # 결측률 낮을수록↑ → (1 - miss)\n",
    "    mi_n   = norm(mi_vals)\n",
    "    f_n    = norm(f_vals)\n",
    "\n",
    "    final  = (miss_n + mi_n + f_n) / 3.0\n",
    "    best_idx = int(np.argmax(final))\n",
    "    rep = scores[best_idx]['col']\n",
    "    rep_map[tuple(pair)] = rep\n",
    "\n",
    "    drop_cols = [c for c in pair if c != rep]\n",
    "    to_drop_pairs.extend(drop_cols)\n",
    "\n",
    "print('[INFO] 대표 선택 결과:', rep_map)\n",
    "print('[INFO] 삭제 대상(대표 아닌 변수):', to_drop_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed51c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train_pair.shape = (21693, 47)\n",
      "[INFO] test_pair.shape  = (15004, 46)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 4. 대표 아닌 변수 삭제(전역) ===\n",
    "train_pair = train.drop(columns=[c for c in to_drop_pairs if c in train.columns], errors='ignore').copy()\n",
    "if test is not None:\n",
    "    test_pair = test.drop(columns=[c for c in to_drop_pairs if c in test.columns], errors='ignore').copy()\n",
    "else:\n",
    "    test_pair = None\n",
    "\n",
    "print('[INFO] train_pair.shape =', train_pair.shape)\n",
    "if test_pair is not None:\n",
    "    print('[INFO] test_pair.shape  =', test_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4502f01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'X_01', 'X_02', 'X_03', 'X_04', 'X_05', 'X_06', 'X_07', 'X_08',\n",
       "       'X_09', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18',\n",
       "       'X_19', 'X_20', 'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28',\n",
       "       'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_40',\n",
       "       'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_51',\n",
       "       'X_52', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37efda8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 파생 피처 추가 완료. train_pair: (21693, 68) | test_pair: (15004, 67)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 파생 피처 추가 (함수 없이 바로 적용) ===\n",
    "_base_cols = [c for c in train_pair.columns if c != TARGET_COL]\n",
    "_num_cols  = train_pair[_base_cols].select_dtypes(include=[float, int]).columns\n",
    "\n",
    "# 1) 행 단위 요약\n",
    "train_pair['row_na_cnt'] = train_pair[_base_cols].isna().sum(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_na_cnt'] = test_pair[_base_cols].isna().sum(axis=1)\n",
    "\n",
    "train_pair['row_num_mean'] = train_pair[_num_cols].mean(axis=1)\n",
    "train_pair['row_num_std']  = train_pair[_num_cols].std(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_num_mean'] = test_pair[_num_cols].mean(axis=1)\n",
    "    test_pair['row_num_std']  = test_pair[_num_cols].std(axis=1)\n",
    "\n",
    "if len(_num_cols) > 0:\n",
    "    _Q1  = train_pair[_num_cols].quantile(0.25)\n",
    "    _Q3  = train_pair[_num_cols].quantile(0.75)\n",
    "    _IQR = (_Q3 - _Q1).replace(0, 1e-12)\n",
    "\n",
    "    _below_tr = (train_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "    _above_tr = (train_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "    train_pair['row_outlier_cnt'] = (_below_tr | _above_tr).sum(axis=1)\n",
    "\n",
    "    if test_pair is not None:\n",
    "        _below_te = (test_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "        _above_te = (test_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "        test_pair['row_outlier_cnt'] = (_below_te | _above_te).sum(axis=1)\n",
    "\n",
    "# 2) G1 그룹 통계\n",
    "_g1_cols = [c for c in G1 if c in train_pair.columns]\n",
    "if len(_g1_cols) > 0:\n",
    "    train_pair['G1_mean']  = train_pair[_g1_cols].mean(axis=1)\n",
    "    train_pair['G1_std']   = train_pair[_g1_cols].std(axis=1)\n",
    "    train_pair['G1_min']   = train_pair[_g1_cols].min(axis=1)\n",
    "    train_pair['G1_max']   = train_pair[_g1_cols].max(axis=1)\n",
    "    train_pair['G1_range'] = train_pair['G1_max'] - train_pair['G1_min']\n",
    "\n",
    "    if test_pair is not None:\n",
    "        test_pair['G1_mean']  = test_pair[_g1_cols].mean(axis=1)\n",
    "        test_pair['G1_std']   = test_pair[_g1_cols].std(axis=1)\n",
    "        test_pair['G1_min']   = test_pair[_g1_cols].min(axis=1)\n",
    "        test_pair['G1_max']   = test_pair[_g1_cols].max(axis=1)\n",
    "        test_pair['G1_range'] = test_pair['G1_max'] - test_pair['G1_min']\n",
    "\n",
    "\n",
    "# 3) 비선형 파생 (예: X_05, X_09)\n",
    "if 'X_05' in train_pair.columns and 'X_09' in train_pair.columns:\n",
    "    train_pair['X05_div_X09'] = train_pair['X_05'] / (train_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X05_mul_X09'] = train_pair['X_05'] * train_pair['X_09']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_div_X09'] = test_pair['X_05'] / (test_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X05_mul_X09'] = test_pair['X_05'] * test_pair['X_09']\n",
    "\n",
    "if 'X_05' in train_pair.columns:\n",
    "    train_pair['X05_sq'] = train_pair['X_05'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_sq'] = test_pair['X_05'] ** 2\n",
    "\n",
    "if 'X_09' in train_pair.columns:\n",
    "    train_pair['X09_sq'] = train_pair['X_09'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X09_sq'] = test_pair['X_09'] ** 2\n",
    "\n",
    "\n",
    "if 'X_20' in train_pair.columns and 'X_22' in train_pair.columns:\n",
    "    train_pair['X20_div_X22'] = train_pair['X_20'] / (train_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X20_mul_X22'] = train_pair['X_20'] * train_pair['X_22']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_div_X22'] = test_pair['X_20'] / (test_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X20_mul_X22'] = test_pair['X_20'] * test_pair['X_22']\n",
    "\n",
    "if 'X_20' in train_pair.columns:\n",
    "    train_pair['X20_sq'] = train_pair['X_20'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_sq'] = test_pair['X_20'] ** 2\n",
    "\n",
    "if 'X_22' in train_pair.columns:\n",
    "    train_pair['X22_sq'] = train_pair['X_22'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X22_sq'] = test_pair['X_22'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "if 'X_25' in train_pair.columns and 'X_51' in train_pair.columns:\n",
    "    train_pair['X25_div_X51'] = train_pair['X_25'] / (train_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X25_mul_X51'] = train_pair['X_25'] * train_pair['X_51']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_div_X51'] = test_pair['X_25'] / (test_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X25_mul_X51'] = test_pair['X_25'] * test_pair['X_51']\n",
    "\n",
    "if 'X_25' in train_pair.columns:\n",
    "    train_pair['X25_sq'] = train_pair['X_25'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_sq'] = test_pair['X_25'] ** 2\n",
    "\n",
    "if 'X_51' in train_pair.columns:\n",
    "    train_pair['X51_sq'] = train_pair['X_51'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X51_sq'] = test_pair['X_51'] ** 2\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# # 1) pairwise 조합 (곱/나눗셈)\n",
    "# for f1, f2 in itertools.combinations(G1, 2):\n",
    "#     if f1 in train_pair.columns and f2 in train_pair.columns:\n",
    "#         # 나눗셈 (f1 / f2)\n",
    "#         train_pair[f\"{f1}_div_{f2}\"] = train_pair[f1] / (train_pair[f2].replace(0, 1e-12) + 1e-12)\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f1}_div_{f2}\"] = test_pair[f1] / (test_pair[f2].replace(0, 1e-12) + 1e-12)\n",
    "\n",
    "#         # 곱 (f1 * f2)\n",
    "#         train_pair[f\"{f1}_mul_{f2}\"] = train_pair[f1] * train_pair[f2]\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f1}_mul_{f2}\"] = test_pair[f1] * test_pair[f2]\n",
    "\n",
    "# # 2) 각 feature에 대해 제곱\n",
    "# for f in G1:\n",
    "#     if f in train_pair.columns:\n",
    "#         train_pair[f\"{f}_sq\"] = train_pair[f] ** 2\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f}_sq\"] = test_pair[f] ** 2\n",
    "\n",
    "print('[INFO] 파생 피처 추가 완료. train_pair:', train_pair.shape, '| test_pair:', (None if test_pair is None else test_pair.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b4e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Fold 1/5] ====="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_1\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ag_cv_models\\fold_1 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       18.60 GB / 31.69 GB (58.7%)\n",
      "Disk Space Avail:   828.68 GB / 953.01 GB (87.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_1\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 68\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 68\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(12), np.int64(4), np.int64(5), np.int64(11)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19040.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 12.50 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.2s = Fit runtime\n",
      "\t66 features in original data used to generate 66 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/18.6 GB\n",
      "\t0.8611\t = Validation score   (f1_macro)\n",
      "\t15.72s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1784.03s of the 1784.03s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/18.6 GB\n",
      "\t0.7878\t = Validation score   (f1_macro)\n",
      "\t10.11s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1773.19s of the 1773.19s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/18.6 GB\n",
      "\t0.8153\t = Validation score   (f1_macro)\n",
      "\t12.24s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1760.12s of the 1760.12s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/18.6 GB\n",
      "\t0.7734\t = Validation score   (f1_macro)\n",
      "\t4.1s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1755.71s of the 1755.71s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/18.2 GB\n",
      "\t0.7633\t = Validation score   (f1_macro)\n",
      "\t9.18s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1746.15s of the 1746.15s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7989\t = Validation score   (f1_macro)\n",
      "\t234.76s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1511.30s of the 1511.29s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/17.0 GB\n",
      "\t0.7426\t = Validation score   (f1_macro)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1509.61s of the 1509.60s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.4 GB\n",
      "\t0.7318\t = Validation score   (f1_macro)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1507.89s of the 1507.89s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8123\t = Validation score   (f1_macro)\n",
      "\t19.95s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1487.59s of the 1487.59s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.9 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8574\t = Validation score   (f1_macro)\n",
      "\t248.2s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1239.33s of the 1239.33s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/18.7 GB\n",
      "\t0.8039\t = Validation score   (f1_macro)\n",
      "\t27.55s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1210.42s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.375, 'XGBoost': 0.125}\n",
      "\t0.8684\t = Validation score   (f1_macro)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 590.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 25420.8 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_1\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_2\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       18.66 GB / 31.69 GB (58.9%)\n",
      "Disk Space Avail:   828.68 GB / 953.01 GB (87.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] macro-F1: 0.8684\n",
      "\n",
      "===== [Fold 2/5] =====\n",
      "[INFO] ag_cv_models\\fold_2 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_2\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 68\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 68\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(20), np.int64(1), np.int64(15), np.int64(0), np.int64(8), np.int64(16), np.int64(14), np.int64(18), np.int64(3), np.int64(5)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19127.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 12.50 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.2s = Fit runtime\n",
      "\t66 features in original data used to generate 66 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.80s of the 1799.80s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/18.7 GB\n",
      "\t0.8604\t = Validation score   (f1_macro)\n",
      "\t18.28s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1781.46s of the 1781.46s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/18.7 GB\n",
      "\t0.7984\t = Validation score   (f1_macro)\n",
      "\t16.84s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1763.19s of the 1763.19s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/18.5 GB\n",
      "\t0.8119\t = Validation score   (f1_macro)\n",
      "\t13.45s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1748.82s of the 1748.82s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/18.6 GB\n",
      "\t0.7783\t = Validation score   (f1_macro)\n",
      "\t4.41s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1744.10s of the 1744.10s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/18.1 GB\n",
      "\t0.7735\t = Validation score   (f1_macro)\n",
      "\t10.59s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1733.09s of the 1733.09s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7973\t = Validation score   (f1_macro)\n",
      "\t244.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1488.48s of the 1488.48s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.3 GB\n",
      "\t0.7511\t = Validation score   (f1_macro)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1486.82s of the 1486.82s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/15.7 GB\n",
      "\t0.7331\t = Validation score   (f1_macro)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1485.12s of the 1485.12s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8088\t = Validation score   (f1_macro)\n",
      "\t27.06s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1457.48s of the 1457.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.3 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8496\t = Validation score   (f1_macro)\n",
      "\t198.43s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1258.97s of the 1258.97s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.7 GB\n",
      "\t0.8072\t = Validation score   (f1_macro)\n",
      "\t26.04s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1231.78s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.444, 'RandomForestGini': 0.222, 'NeuralNetTorch': 0.222, 'RandomForestEntr': 0.111}\n",
      "\t0.8657\t = Validation score   (f1_macro)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 568.73s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 13420.5 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_2\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.89 GB / 31.69 GB (53.3%)\n",
      "Disk Space Avail:   828.72 GB / 953.01 GB (87.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] macro-F1: 0.8657\n",
      "\n",
      "===== [Fold 3/5] =====\n",
      "[INFO] ag_cv_models\\fold_3 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_3\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 68\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 68\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17259.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 12.50 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.2s = Fit runtime\n",
      "\t66 features in original data used to generate 66 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.81s of the 1799.80s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.8 GB\n",
      "\t0.8648\t = Validation score   (f1_macro)\n",
      "\t15.99s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1783.76s of the 1783.76s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/16.8 GB\n",
      "\t0.794\t = Validation score   (f1_macro)\n",
      "\t14.3s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1768.22s of the 1768.22s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/16.8 GB\n",
      "\t0.8152\t = Validation score   (f1_macro)\n",
      "\t11.05s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1756.47s of the 1756.47s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.8 GB\n",
      "\t0.7743\t = Validation score   (f1_macro)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1751.97s of the 1751.96s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.3 GB\n",
      "\t0.771\t = Validation score   (f1_macro)\n",
      "\t10.42s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1741.21s of the 1741.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8023\t = Validation score   (f1_macro)\n",
      "\t233.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1507.50s of the 1507.50s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/17.0 GB\n",
      "\t0.7549\t = Validation score   (f1_macro)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1505.82s of the 1505.82s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.3 GB\n",
      "\t0.7349\t = Validation score   (f1_macro)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1504.10s of the 1504.10s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8114\t = Validation score   (f1_macro)\n",
      "\t14.68s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1489.13s of the 1489.12s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.6 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 5. Stratified K-Fold CV (fold별로 G1 표준화+PCA 적용 후 AutoGluon 학습) ===\n",
    "if not AUTOGluon_OK:\n",
    "    raise ImportError('AutoGluon 불러오기 실패. 설치 필요: pip install autogluon.tabular')\n",
    "\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_indices = list(skf.split(train_pair.drop(columns=[TARGET_COL]), train_pair[TARGET_COL].astype(str)))\n",
    "\n",
    "oof_pred = pd.Series(index=np.arange(len(train_pair)), dtype=object)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(fold_indices, start=1):\n",
    "    print(f'\\n===== [Fold {fold}/{N_FOLDS}] =====')\n",
    "    trn_df = train_pair.iloc[trn_idx].reset_index(drop=True).copy()\n",
    "    val_df = train_pair.iloc[val_idx].reset_index(drop=True).copy()\n",
    "\n",
    "    # # --- G1 처리: 표준화 → PCA(0.95) → PC_G1_* 생성, G1 원본 삭제 ---\n",
    "    # g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    # if len(g1_cols) > 0:\n",
    "    #     # 결측 채우기(수치 median) → 스케일러/ PCA 학습은 trn 기준\n",
    "    #     imputer_g1 = SimpleImputer(strategy='median')\n",
    "    #     scaler_g1  = StandardScaler()\n",
    "    #     pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "    #     trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "    #     trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "    #     trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "    #     # 검증 변환\n",
    "    #     val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "    #     val_g1 = scaler_g1.transform(val_g1)\n",
    "    #     val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "    #     # PC 열명 만들기\n",
    "    #     n_pc = trn_g1_pc.shape[1]\n",
    "    #     pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "    #     # 데이터프레임에 붙이고 원본 G1 삭제\n",
    "    #     trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols)\n",
    "    #     val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols)\n",
    "\n",
    "    #     trn_df = pd.concat([trn_df.drop(columns=g1_cols), trn_pc_df], axis=1)\n",
    "    #     val_df = pd.concat([val_df.drop(columns=g1_cols), val_pc_df], axis=1)\n",
    "    # else:\n",
    "    #     print('[INFO] 이번 fold에서 G1 컬럼이 존재하지 않아 PCA 스킵')\n",
    "\n",
    "\n",
    "\n",
    "    # --- G1 처리: 표준화 → PCA(0.95) → PC_G1_* 생성, G1 원본 유지 ---\n",
    "    g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    if len(g1_cols) > 0:\n",
    "        # 결측 채우기(수치 median) → 스케일러/ PCA 학습은 trn 기준\n",
    "        imputer_g1 = SimpleImputer(strategy='median')\n",
    "        scaler_g1  = StandardScaler()\n",
    "        pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "        trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "        trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "        # 검증 변환\n",
    "        val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "        val_g1 = scaler_g1.transform(val_g1)\n",
    "        val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "        # PC 열명 만들기\n",
    "        n_pc = trn_g1_pc.shape[1]\n",
    "        pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "        # 데이터프레임에 붙이고 원본 G1 유지\n",
    "        trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols, index=trn_df.index)\n",
    "        val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols, index=val_df.index)\n",
    "\n",
    "        trn_df = pd.concat([trn_df, trn_pc_df], axis=1)\n",
    "        val_df = pd.concat([val_df, val_pc_df], axis=1)\n",
    "    else:\n",
    "        print('[INFO] 이번 fold에서 G1 컬럼이 존재하지 않아 PCA 스킵')\n",
    "\n",
    "\n",
    "\n",
    "    # AutoGluon 학습\n",
    "    fold_dir = SAVE_ROOT / f'fold_{fold}'\n",
    "    if fold_dir.exists():\n",
    "        print(f'[INFO] {fold_dir} 재사용/덮어쓰기')\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ag_trn = TabularDataset(trn_df)\n",
    "    ag_val = TabularDataset(val_df)\n",
    "\n",
    "    predictor = TabularPredictor(label=TARGET_COL, path=str(fold_dir), eval_metric='f1_macro')\n",
    "    predictor.fit(train_data=ag_trn, tuning_data=ag_val, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # 검증 예측\n",
    "    y_true = val_df[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "    y_pred = predictor.predict(ag_val).astype(str).reset_index(drop=True)\n",
    "\n",
    "    score = f1_score(y_true, y_pred, average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "    print(f'[Fold {fold}] macro-F1: {score:.4f}')\n",
    "    fold_scores.append(score)\n",
    "\n",
    "    oof_pred.iloc[val_idx] = y_pred.values\n",
    "\n",
    "# OOF 성능\n",
    "y_all = train_pair[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "assert oof_pred.isna().sum() == 0, 'OOF 예측에 결측이 있습니다.'\n",
    "\n",
    "oof_score = f1_score(y_all, oof_pred.astype(str), average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "print('\\n===== CV 결과 =====')\n",
    "print('Fold scores:', np.round(fold_scores, 4).tolist())\n",
    "print(f'OOF macro-F1: {oof_score:.4f}')\n",
    "\n",
    "oof_df = pd.DataFrame({'y_true': y_all, 'y_pred': oof_pred.astype(str)})\n",
    "oof_df.to_csv(OOF_PATH, index=False, encoding='utf-8')\n",
    "print('[INFO] OOF 저장:', OOF_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_full_model\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       17.39 GB / 31.69 GB (54.9%)\n",
      "Disk Space Avail:   831.00 GB / 953.01 GB (87.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_full_model\"\n",
      "Train Data Rows:    21693\n",
      "Train Data Columns: 68\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17805.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 12.50 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 65 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.2s = Fit runtime\n",
      "\t66 features in original data used to generate 66 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 19523, Val Rows: 2170\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.81s of the 1799.81s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/17.4 GB\n",
      "\t0.8521\t = Validation score   (f1_macro)\n",
      "\t17.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1782.60s of the 1782.60s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/17.3 GB\n",
      "\t0.7843\t = Validation score   (f1_macro)\n",
      "\t12.61s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1769.04s of the 1769.04s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/17.3 GB\n",
      "\t0.8001\t = Validation score   (f1_macro)\n",
      "\t8.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1760.56s of the 1760.56s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/17.1 GB\n",
      "\t0.7612\t = Validation score   (f1_macro)\n",
      "\t4.67s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1755.56s of the 1755.56s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/16.7 GB\n",
      "\t0.763\t = Validation score   (f1_macro)\n",
      "\t10.27s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1744.97s of the 1744.97s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7913\t = Validation score   (f1_macro)\n",
      "\t268.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1476.73s of the 1476.73s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/15.8 GB\n",
      "\t0.7372\t = Validation score   (f1_macro)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1474.97s of the 1474.97s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/15.1 GB\n",
      "\t0.7276\t = Validation score   (f1_macro)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1473.10s of the 1473.10s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7995\t = Validation score   (f1_macro)\n",
      "\t19.13s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1453.69s of the 1453.69s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/15.5 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8485\t = Validation score   (f1_macro)\n",
      "\t387.38s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1066.26s of the 1066.26s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.9 GB\n",
      "\t0.7995\t = Validation score   (f1_macro)\n",
      "\t30.37s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1034.47s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.357, 'XGBoost': 0.214, 'NeuralNetTorch': 0.214, 'RandomForestGini': 0.071, 'ExtraTreesGini': 0.071, 'ExtraTreesEntr': 0.071}\n",
      "\t0.8571\t = Validation score   (f1_macro)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 765.92s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6433.6 rows/s (2170 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_full_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] submission 저장: C:\\Users\\SSAFY\\Downloads\\Autogluon\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 6. Full-train + Test 예측 (전역 대표/드롭 적용 기준, G1은 full-train 기준으로 PCA) ===\n",
    "if test is None:\n",
    "    print('[WARN] test.csv 없음 → 스킵')\n",
    "else:\n",
    "    # 전역 pair-drop 반영된 사본 사용\n",
    "    trn_full = train_pair.copy()\n",
    "    tst_full = test_pair.copy()\n",
    "\n",
    "    # # G1 처리: 결측→표준화→PCA(0.95) → PC_G1_* 남기고 원본 삭제 (full-train 기준으로 적합)\n",
    "    # g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    # if len(g1_cols_full) > 0:\n",
    "    #     imputer_g1_full = SimpleImputer(strategy='median')\n",
    "    #     scaler_g1_full  = StandardScaler()\n",
    "    #     pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "    #     trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "    #     trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "    #     trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "    #     tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "    #     tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "    #     tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "    #     n_pc_full = trn_g1f_pc.shape[1]\n",
    "    #     pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "    #     trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full)\n",
    "    #     tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full)\n",
    "\n",
    "    #     trn_full = pd.concat([trn_full.drop(columns=g1_cols_full), trn_pc_full], axis=1)\n",
    "    #     tst_full = pd.concat([tst_full.drop(columns=g1_cols_full), tst_pc_full], axis=1)\n",
    "    # else:\n",
    "    #     print('[INFO] full-train 기준 G1 컬럼 없음 → PCA 스킵')\n",
    "\n",
    "\n",
    "    # G1 처리: 결측→표준화→PCA(0.95) → PC_G1_* 추가 (원본은 유지)\n",
    "    g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    if len(g1_cols_full) > 0:\n",
    "        imputer_g1_full = SimpleImputer(strategy='median')\n",
    "        scaler_g1_full  = StandardScaler()\n",
    "        pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        # train\n",
    "        trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "        trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "        trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "        # test\n",
    "        tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "        tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "        tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "        # PC 컬럼 생성\n",
    "        n_pc_full = trn_g1f_pc.shape[1]\n",
    "        pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "        trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full, index=trn_full.index)\n",
    "        tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full, index=tst_full.index)\n",
    "\n",
    "        # 원본은 유지하고 PC만 추가\n",
    "        trn_full = pd.concat([trn_full, trn_pc_full], axis=1)\n",
    "        tst_full = pd.concat([tst_full, tst_pc_full], axis=1)\n",
    "    else:\n",
    "        print('[INFO] full-train 기준 G1 컬럼 없음 → PCA 스킵')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # AutoGluon 전체 학습\n",
    "    FULL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ag_full = TabularDataset(trn_full)\n",
    "    full_predictor = TabularPredictor(label=TARGET_COL, path=str(FULL_DIR), eval_metric='f1_macro')\n",
    "    full_predictor.fit(train_data=ag_full, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # Test 예측 & 제출 저장\n",
    "    ag_test = TabularDataset(tst_full)\n",
    "    test_pred = full_predictor.predict(ag_test).astype(str)\n",
    "\n",
    "    if ID_COL is not None and ID_COL in test.columns:\n",
    "        sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET_COL: test_pred})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'row_id': np.arange(len(test_pred)), TARGET_COL: test_pred})\n",
    "\n",
    "    sub.to_csv(SUB_PATH, index=False, encoding='utf-8')\n",
    "    print('[INFO] submission 저장:', SUB_PATH.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9351c1",
   "metadata": {},
   "source": [
    "\n",
    "## 전처리 메모 (요약)\n",
    "- **PAIR_GROUPS 대표 선택**: `(-결측률) 정규화 + MI 정규화 + F 정규화` 평균으로 대표 1개 선택(전역 고정). 대표 외 변수는 **삭제**.\n",
    "- **G1 그룹**: 수치 결측 median 대체 → 표준화(`StandardScaler`) → `PCA(n_components=0.95)` → `PC_G1_i`만 유지, G1 원본 **삭제**.\n",
    "- **K-Fold 내 누수 방지**: G1의 `imputer/scaler/PCA`는 **각 폴드의 train 부분으로만 적합** 후 val에 적용. (전역 pair-drop은 기준 통일을 위해 train 전체에서 1회 산정)\n",
    "- AutoGluon은 나머지 결측/범주형 처리를 자체적으로 수행.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
